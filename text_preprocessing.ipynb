{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDF/r7s9IqnJcf8pDpIYBj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harika-Mullaguri/Deep-Learning/blob/main/text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p05ZP88y7xHd",
        "outputId": "72afc5d4-f144-4b91-82ad-f0142db362b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "with open('/content/1661-0.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Filter out potential metadata lines or empty lines at the beginning/end if needed\n",
        "# For simplicity, let's just put all non-empty lines into the DataFrame for now.\n",
        "# More sophisticated parsing might be needed based on the actual book content.\n",
        "book_content = [line.strip() for line in lines if line.strip()]\n",
        "df = pd.DataFrame(book_content, columns=['sentences'])\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-kjM8i_8HVe",
        "outputId": "a521cfe2-0d97-4b08-8e27-2ab7ba04af7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           sentences\n",
            "0                                                  ﻿\n",
            "1  Project Gutenberg's The Adventures of Sherlock...\n",
            "2  This eBook is for the use of anyone anywhere a...\n",
            "3  almost no restrictions whatsoever.  You may co...\n",
            "4  re-use it under the terms of the Project Guten...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Convert 'sentences' column to string type, then to lowercase\n",
        "df['normalized_text'] = df['sentences'].astype(str).str.lower()\n",
        "\n",
        "# Remove punctuation from 'normalized_text'\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "df['normalized_text'] = df['normalized_text'].str.translate(translator)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJByAHMS8fC2",
        "outputId": "e08ebcfa-4cc6-4ae8-a17b-ec787c3af21e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           sentences  \\\n",
            "0                                                  ﻿   \n",
            "1  Project Gutenberg's The Adventures of Sherlock...   \n",
            "2  This eBook is for the use of anyone anywhere a...   \n",
            "3  almost no restrictions whatsoever.  You may co...   \n",
            "4  re-use it under the terms of the Project Guten...   \n",
            "\n",
            "                                     normalized_text  \n",
            "0                                                  ﻿  \n",
            "1  project gutenbergs the adventures of sherlock ...  \n",
            "2  this ebook is for the use of anyone anywhere a...  \n",
            "3  almost no restrictions whatsoever  you may cop...  \n",
            "4  reuse it under the terms of the project gutenb...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_text = df['normalized_text'].apply(nltk.word_tokenize)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = tokenized_text.apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_text = filtered_text.apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "\n",
        "df['normalized_text'] = lemmatized_text.apply(' '.join)\n",
        "print(df[['sentences', 'normalized_text']])\n",
        "\n",
        "# Save the DataFrame to a new Excel file\n",
        "df.to_excel('output_file_preprocessed.xlsx', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppwvv46S8Ylm",
        "outputId": "423942ec-b0ca-4017-a380-a927fd30a452"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              sentences  \\\n",
            "0                                                     ﻿   \n",
            "1     Project Gutenberg's The Adventures of Sherlock...   \n",
            "2     This eBook is for the use of anyone anywhere a...   \n",
            "3     almost no restrictions whatsoever.  You may co...   \n",
            "4     re-use it under the terms of the Project Guten...   \n",
            "...                                                 ...   \n",
            "9629                        facility: www.gutenberg.org   \n",
            "9630  This Web site includes information about Proje...   \n",
            "9631  including how to make donations to the Project...   \n",
            "9632  Archive Foundation, how to help produce our ne...   \n",
            "9633  subscribe to our email newsletter to hear abou...   \n",
            "\n",
            "                                        normalized_text  \n",
            "0                                                     ﻿  \n",
            "1     project gutenberg adventure sherlock holmes ar...  \n",
            "2                        ebook use anyone anywhere cost  \n",
            "3      almost restriction whatsoever may copy give away  \n",
            "4         reuse term project gutenberg license included  \n",
            "...                                                 ...  \n",
            "9629                           facility wwwgutenbergorg  \n",
            "9630  web site includes information project gutenbergtm  \n",
            "9631  including make donation project gutenberg lite...  \n",
            "9632         archive foundation help produce new ebooks  \n",
            "9633         subscribe email newsletter hear new ebooks  \n",
            "\n",
            "[9634 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3HhqkIiQ9Wfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}